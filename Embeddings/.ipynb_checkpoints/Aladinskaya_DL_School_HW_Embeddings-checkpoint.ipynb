{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot3c4fjZwC4T"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OWVtaMohyat"
   },
   "source": [
    "# New section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2JdzEXmwRU5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYtJxkhKpYK2"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Привет! В этом домашнем задании мы с помощью эмбеддингов решим задачу семантической классификации твитов.\n",
    "\n",
    "Для этого мы воспользуемся предобученными эмбеддингами word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBOdoFS8AdpP"
   },
   "source": [
    "Для начала скачаем датасет для семантической классификации твитов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wXjhtsfF_gBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph\n",
      "Archive:  archive.zip\n",
      "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n",
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh6wW-K53Mle"
   },
   "source": [
    "Импортируем нужные библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2Y5CHRm6NFe"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "73Lb0wbESrgQ"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed_all(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_Wv-4bu83Fl"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY1pvYDS3Yuj"
   },
   "source": [
    "Посмотрим на данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jST2tjgjCTWD"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbR5JJyA2VW"
   },
   "source": [
    "Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCBwe0wR83C2"
   },
   "outputs": [],
   "source": [
    "examples = data[\"text\"].sample(10)\n",
    "print(\"\\n\".join(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvcYW8aX3mKt"
   },
   "source": [
    "Как вилим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n",
    "\n",
    "Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8hUK-jnQg6O"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(data.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "dev_size = math.ceil(data.shape[0] * 0.8)\n",
    "\n",
    "dev_indexes = indexes[:dev_size]\n",
    "test_indexes = indexes[dev_size:]\n",
    "\n",
    "dev_data = data.iloc[dev_indexes]\n",
    "test_data = data.iloc[test_indexes]\n",
    "\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ivcpeFoCnZA"
   },
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df4nca285Dar"
   },
   "source": [
    "Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nsNHNDES9ZVF"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n",
    "print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcBS_u_hTuxp"
   },
   "outputs": [],
   "source": [
    "filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
    "print(\" \".join(filtered_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuFmlXkC6E7X"
   },
   "source": [
    "Загрузим предобученную модель эмбеддингов. \n",
    "\n",
    "Если хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n",
    "\n",
    "Данная модель выдает эмбеддинги для **слов**. Строить по эмбеддингам слов эмбеддинги предложений мы будем ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cACJpje2T5bc"
   },
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NafmYHrkT5YD"
   },
   "outputs": [],
   "source": [
    "emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n",
    "print(sum(emb_line).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTS6LCkd6_E7"
   },
   "source": [
    "Нормализуем эмбеддинги, прежде чем обучать на них сеть. \n",
    "(наверное, вы помните, что нейронные сети гораздо лучше обучаются на нормализованных данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PyLTZ6xf3Oq"
   },
   "outputs": [],
   "source": [
    "mean = np.mean(word2vec.vectors, 0)\n",
    "std = np.std(word2vec.vectors, 0)\n",
    "norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n",
    "print(sum(norm_emb_line).shape)\n",
    "print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7vm6Ppd7Ubw"
   },
   "source": [
    "Сделаем датасет, который будет по запросу возвращать подготовленные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4eZajF7pZ1X"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        # Получи все токены из текста и профильтруй их\n",
    "        tokens_raw = tokenizer.tokenize(text.lower())\n",
    "        tokens_filt = [w for w in tokens_raw if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
    "        return tokens_filt\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        embeddings = [(self.word2vec.get_vector(w) - self.mean) / self.std for w in tokens if w in self.word2vec] # Получи эмбеддинги слов и усредни их\n",
    "\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZJpttbXpZyz"
   },
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr-aetH0_LH1"
   },
   "source": [
    "Отлично, мы готовы с помощью эмбеддингов слов превращать твиты в векторы и обучать нейронную сеть.\n",
    "\n",
    "Превращать твиты в векторы, используя эмбеддинги слов, можно несколькими способами. А именно такими:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AhHrWa196Yc"
   },
   "source": [
    "## Average embedding (2 балла)\n",
    "---\n",
    "Это самый простой вариант, как получить вектор предложения, используя векторные представления слов в предложении. А именно: вектор предложения есть средний вектор всех слов в предлоежнии (которые остались после токенизации и удаления коротких слов, конечно). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScdokSW-994t"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(dev))\n",
    "np.random.shuffle(indexes)\n",
    "example_indexes = indexes[::1000]\n",
    "\n",
    "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
    "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
    "print(len(examples[\"features\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yGQ_lOx_1NL"
   },
   "source": [
    "Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета. Так мы увидим, насколько хорошо твиты с разными target значениями отделяются друг от друга, т.е. насколько хорошо усреднение эмбеддингов слов предложения передает информацию о предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZwFksd_8uYO"
   },
   "source": [
    "Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Если хотите, можете вместо PCA использовать TSNE: так у вас получится более точная проекция на плоскость (а значит, более информативная, т.е. отражающая реальное положение векторов твитов в пространстве). Но TSNE будет работать намного дольше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKFZRSHdtIac"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE()\n",
    "examples[\"transformed_features\"] = tsne.fit_transform(examples['features'])\n",
    "#examples[\"transformed_features\"] = pca.fit_transform(examples['features']) # Обучи PCA на эмбеддингах слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szEOWdiNtIX8"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OONK8ldtIWe"
   },
   "outputs": [],
   "source": [
    "draw_vectors(\n",
    "    examples[\"transformed_features\"][:, 0], \n",
    "    examples[\"transformed_features\"][:, 1], \n",
    "    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fNF6LRQ9MPI"
   },
   "source": [
    "Скорее всего, на визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n",
    "\n",
    "Подготовим загрузчики данных.\n",
    "Усреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1XapsADtITv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-zs0WEK-Vkt"
   },
   "source": [
    "Определим функции для тренировки и теста модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U--T2Gjw1r27"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        features = batch[\"features\"].to(device)\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Получи предсказания модели\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs,targets) # Посчитай лосс\n",
    "        loss.backward() # Обнови параметры модели\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}\")\n",
    "    \n",
    "\n",
    "def testing(model, criterion, test_loader, device=\"cpu\"):\n",
    "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            # Получи предсказания модели\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets) # Посчитай лосс\n",
    "             # Посчитай точность модели\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            acc = (preds == targets).to(torch.float).mean()   \n",
    "\n",
    "            mean_loss += loss.item()\n",
    "            mean_acc += acc.item()\n",
    "\n",
    "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
    "\n",
    "    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
    "\n",
    "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVg_XBBb-YBH"
   },
   "source": [
    "Создадим модель, оптимизатор и целевую функцию. Вы можете сами выбрать количество слоев в нейронной сети, ваш любимый оптимизатор и целевую функцию.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBoZ4F3Fx1Hm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Не забудь поиграться с параметрами ;)\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 2\n",
    "lr = 1e-2\n",
    "num_epochs = 1\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(vector_size, 100),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(100, 200),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(200, num_classes)\n",
    ")\n",
    "#model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AitU8AR-zBj"
   },
   "source": [
    "Наконец, обучим модель и протестируем её.\n",
    "\n",
    "После каждой эпохи будем проверять качество модели на валидационной части датасета. Если метрика стала лучше, будем сохранять модель. **Подумайте, какая метрика (точность или лосс) будет лучше работать в этой задаче?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKhk71Pmx1F1"
   },
   "outputs": [],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "di4dGwD4x1Dt"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec), \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6hsNiF-gahu"
   },
   "source": [
    "Ответ: Loss для оптимизации будет лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRvzpldHSAu0"
   },
   "source": [
    "## Embeddings for unknown words (8 баллов)\n",
    "\n",
    "Пока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n",
    "\n",
    "- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n",
    "- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)\n",
    "\n",
    "Реализуйте оба варианта **ниже**. Напишите, какой способ сработал лучше и ваши мысли, почему так получилось."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCCkVURLQTBM"
   },
   "source": [
    "1. Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWrIhCiBpLjU"
   },
   "outputs": [],
   "source": [
    "#Размер окна можно задать любым по желанию. Я выбрала размер - 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ0FzTxUBeub"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TwitterDatasetEmbeddings(TwitterDataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec, window_size: int):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "\n",
    "      embeddings = np.zeros((len(tokens), self.word2vec.vector_size))\n",
    "\n",
    "      for idx, tok in enumerate(tokens):\n",
    "        if tok in self.word2vec:\n",
    "          emb = (self.word2vec.get_vector(tok) - self.mean) / self.std\n",
    "          embeddings[idx] = emb\n",
    "        else:\n",
    "          tokens[idx] = '<UNK>'\n",
    "\n",
    "      for idx, tok in enumerate(tokens):\n",
    "\n",
    "        if tok == '<UNK>':\n",
    "\n",
    "          start_idx = max(0, idx - self.window_size)\n",
    "          end_idx = min(len(tokens), idx + self.window_size +1)\n",
    "          pos_in_window = self.window_size\n",
    "\n",
    "          context_inds = np.array([int(ind) for ind in range(start_idx, end_idx) if ind != idx])\n",
    "\n",
    "          if idx - self.window_size < 0:  # start of the sentence\n",
    "            pos_in_window += idx - self.window_size\n",
    "\n",
    "          context = tokens[start_idx:end_idx]  # cuts window from sentence\n",
    "          context = np.delete(context, pos_in_window)\n",
    "          \n",
    "\n",
    "          emb = np.zeros(self.word2vec.vector_size)\n",
    "\n",
    "          for word in context:\n",
    "            if word != '<UNK>':\n",
    "              value = (self.word2vec.get_vector(word)- self.mean) / self.std\n",
    "              emb += value\n",
    "            else:\n",
    "              emb += np.zeros(self.word2vec.vector_size)\n",
    "\n",
    "          embeddings[idx] = emb\n",
    "\n",
    "        else:\n",
    "          value = (self.word2vec.get_vector(tok)- self.mean) / self.std\n",
    "          embeddings[idx] = value\n",
    "        \n",
    "      if len(embeddings) == 0:\n",
    "          embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "      else:\n",
    "          embeddings = np.array(embeddings)\n",
    "          if len(embeddings.shape) == 1:\n",
    "              embeddings = embeddings.reshape(-1, 1)\n",
    "        \n",
    "      return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKHwbTCVK6m0"
   },
   "outputs": [],
   "source": [
    "dev = TwitterDatasetEmbeddings(\n",
    "    data=dev_data,\n",
    "    feature_column=\"text\",\n",
    "    target_column=\"emotion\",\n",
    "    word2vec=word2vec,\n",
    "    window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeHQN6g4QFgy"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(dev))\n",
    "np.random.shuffle(indexes)\n",
    "example_indexes = indexes[::1000]\n",
    "\n",
    "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
    "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
    "print(len(examples[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLZqUeJ6xIxl"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SxZNuPUxNnx"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Не забудь поиграться с параметрами ;)\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 2\n",
    "lr = 1e-2\n",
    "num_epochs = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(vector_size, 100),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(100, 200),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(200, num_classes)\n",
    ")\n",
    "#model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5NGVaOdxvRn"
   },
   "outputs": [],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OrCE6ssxvTd"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDatasetEmbeddings(data=test_data,feature_column=\"text\",target_column=\"emotion\",word2vec=word2vec,window_size=3), \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deZOsQxVQYeK"
   },
   "source": [
    "2. Для каждого слова текста получим его эмбеддинг из Tfidf с помощью TfidfVectorizer из sklearn. Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmzi5xje5o6Z"
   },
   "outputs": [],
   "source": [
    "! pip install sparsesvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KDKEpdRf1v6"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sparsesvd import sparsesvd\n",
    "import gensim.downloader as gensim_downloader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S43qh3fR-llK"
   },
   "outputs": [],
   "source": [
    "def out_get_tokens_(text, tokenizer=nltk.WordPunctTokenizer()):\n",
    "    # Получи все токены из текста и профильтруй их\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    filtered_tokens = [t for t in tokens if all(c not in string.punctuation for c in t) and len(t) > 3]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlybZWO2b9ce"
   },
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(tokenizer = out_get_tokens_, min_df=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsOfAWzU-Fwj"
   },
   "outputs": [],
   "source": [
    "matrix_tf = tf.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwXkoEEZCIew"
   },
   "outputs": [],
   "source": [
    "V, Sigma, UT = sparsesvd(matrix_tf.tocsc(), 300) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMAz0T16fGJ9"
   },
   "outputs": [],
   "source": [
    "k,v = tf.get_feature_names(), UT.T\n",
    "dict_tf = {k[i] : v[i] for i in range(len(v))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwcwcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyO9rFSIk-yz"
   },
   "outputs": [],
   "source": [
    "with open('dict_tf.pickle', 'wb') as handle:\n",
    "   pickle.dump(dict_tf, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4N352iJjvup"
   },
   "outputs": [],
   "source": [
    "#RAM не хватает... выгрузила словарь ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rK5joIB3k_ME"
   },
   "outputs": [],
   "source": [
    "#with open('tf_dict.pickle', 'wb') as handle:\n",
    "#   pickle.dump(tf_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvTv698ourhs"
   },
   "outputs": [],
   "source": [
    "# Доп можно сохранить на компьютер\n",
    "#from google.colab import files\n",
    "#files.download('tf_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMCmD4z-ZgDd"
   },
   "outputs": [],
   "source": [
    "# выгрузить словарь - сохранить RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29rgfg_cwdNW"
   },
   "outputs": [],
   "source": [
    "# При перезапуске, загружаем словарь обратно\n",
    "with open('dict_tf.pickle', 'rb') as handle:\n",
    "    dict_tf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jag0MtDIdn3W"
   },
   "outputs": [],
   "source": [
    "print(word2vec[ 'method'].shape, dict_tf['word'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpLsp6fm1cun"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec, dict_tf):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "        self.dict_tf = dict_tf\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        # Получи все токены из текста и профильтруй их\n",
    "        tokens_raw = tokenizer.tokenize(text.lower())\n",
    "        tokens_filt = [w for w in tokens_raw if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
    "        return tokens_filt\n",
    "\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "    \n",
    "      embeddings = np.zeros((len(tokens), self.word2vec.vector_size))\n",
    "\n",
    "      for i, tok in enumerate(tokens):\n",
    "\n",
    "        if tok in self.dict_tf.keys():\n",
    "            if tok in self.word2vec:\n",
    "              emb_tf = self.dict_tf.get(tok)\n",
    "              emb_w2v = self.word2vec.get_vector(tok)\n",
    "              emb = emb_tf + emb_w2v\n",
    "              embeddings[i] = emb\n",
    "            else:\n",
    "              emb_tf = self.dict_tf.get(tok)\n",
    "              embeddings[i] = emb_tf\n",
    "      \n",
    "      if len(embeddings) == 0:\n",
    "          embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "      else:\n",
    "          embeddings = np.array(embeddings)\n",
    "          if len(embeddings.shape) == 1:\n",
    "              embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "      return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a6ze8svoUiw"
   },
   "outputs": [],
   "source": [
    "dev = TwitterDataset(\n",
    "    data=dev_data,\n",
    "    feature_column=\"text\", target_column=\"emotion\",\n",
    "    word2vec=word2vec,\n",
    "    dict_tf=dict_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfmXbI99xanL"
   },
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec, dict_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WMjaHz9QObw"
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(len(dev))\n",
    "np.random.shuffle(indexes)\n",
    "example_indexes = indexes[::1000]\n",
    "\n",
    "examples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
    "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
    "print(len(examples[\"features\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNYdTv2UxvVz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tIGUNUwxkCE"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "# Не забудь поиграться с параметрами ;)\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 2\n",
    "lr = 1e-2\n",
    "num_epochs = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(vector_size, 100),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(100, 200),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(200, num_classes)\n",
    ")\n",
    "#model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdubgbTJraLv"
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "device\n",
    "#Были проблемы с тем что что-то на CPU, что-то на CUDa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7dWs_0UxkEM"
   },
   "outputs": [],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EF1ORlsGxkGQ"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec,\n",
    "                                  dict_tf=dict_tf), \n",
    "    batch_size=batch_size, \n",
    "    num_workers=num_workers, \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi8kaFg35fu2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Fph-7ZlvwwR"
   },
   "source": [
    "Выводы\n",
    "\n",
    "По итогу лучшее всего себя показал последний метод. Добавить просчитанные на конкретном примере векторы - более информативно. \n",
    "Между методом с эмбеддингами и средним разница не столь велика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2BP4zHlxkMC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Copy of [homework]embeddings.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
